---
title: "Statistics Pipeline"
## for html
#output:
#  html_notebook:
#    toc: yes
#    toc_float:
#      smooth_scroll: yes
#      collapsed: no
#  html_document:
#    toc: yes
#    df_print: paged
## for github markdown
output: rmarkdown::github_document
date: "Last edited: `r format(Sys.time(), '%Y-%m-%d')`"

---
```{r tidyverse_load, include=F}
library(tidyverse)
library(kableExtra)
library(rmarkdown)
```

```{r dataframe, include=F}
Target <- rep("Target gene", times = 12)
Condition <- factor(rep(c("Control", "Condition 1","Condition 2", "Condition 3"), each = 3), levels = c("Control", "Condition 1","Condition 2", "Condition 3"))
Value_normalised <- c(
  1.070688086, 1.084484165, 0.944827749, 81.20096936, 64.32041674, 75.01759177, 24.90206996, 19.923147271,19.3690698, 278.2460496, 292.1552481, 240.35566374
)
dataset_multiple <- data.frame(Target, Condition, Value_normalised)
dataset_single <- dataset_multiple %>% filter(Condition %in% c("Control", "Condition 1"))
```

# Introduction
This workbook acts as a template for statistical analysis. it provides guidelines for assumptions and test to use for multiple experimental conditions and single experimental condition.

Below is the sample data for multiple experimental conditions:
```{r dataset_multiple}
dataset_multiple |> kable()
```


Below is the sample data for one experimental condition:
```{r dataset_single}
dataset_single |> kable()
```

# Testing Statistical Assumptions

## Visual Tests

#### Normal Distribution by Boxplot

```{r boxplot}
boxplot(Value_normalised~Condition, dataset_multiple)
```

#### Testing equality of variance assumptions

```{r testing_assumptions}
plot(lm(Value_normalised~Condition, dataset_multiple))
```

**1st and the last plots:** we want symmetrical data about the 0 horizontal line

**2nd plot:** we want residual points to be as close to the predicted line as possible

**3rd plot:** we want for red line to be approx. horizontal

## Mathematical tests

### Testing Normality

#### Test Normality (of distribution) per treatment group

**p value \> 0.05 means normal distribution**

```{r shapiro.test_single}
shapiro.test(dataset_multiple$Value_normalised[1:3]) # test all values in one condition
shapiro.test(dataset_multiple$Value_normalised[4:6]) # test all values in one condition
shapiro.test(dataset_multiple$Value_normalised[7:9]) # test all values in one condition
shapiro.test(dataset_multiple$Value_normalised[10:12]) # test all values in one condition
```

#### Test Normality (of distribution) for the whole dataset

Visual assessment where you want the residual points to be as close line as possible.

```{r shapiro.test_total}
plot(residuals(lm(Value_normalised~Condition, dataset_multiple))) # test all values in the whole dataset
shapiro.test(residuals(lm(Value_normalised~Condition, dataset_multiple))) # test all values in the whole dataset
```

If all groups and combined analysis evaluate to normal distribution then its normal. If one group is non normal by itself but combined dataset evaluates to normal distribution usually you can assume its normal distribution.

### Testing the Equality of Variance

#### For **NORMAL** Distribution

**p value \> 0.05 means equal variance**

```{r bartlett.test}
bartlett.test(Value_normalised~Condition, dataset_multiple)
```

#### For **NON NORMAL** Distribution

**p value \> 0.05 means equal variance**

```{r leveneTest}
library(car, quietly = TRUE)
leveneTest(Value_normalised~Condition, dataset_multiple)
```

# Obtaining Statistical Parameters

## For **Normal Distribution** and **Equal Variance**

### Multiple Comparisons

```{r anova}
aov(lm(Value_normalised~Condition, dataset_multiple))
```

Anova finds there is any significant difference across the whole dataset. If the p-value (Pr(>F) is **ABOVE** 0.05 the analysis should be stopped here!

Important parameters: F value and Pr(>F) (include in reports)

```{r tukey}
TukeyHSD(aov(Value_normalised~Condition, dataset_multiple))
```

P adj are the individual p values between group combinations.

### Single Comparison

```{r paired_t_test}
t.test(Value_normalised~Condition, 
       data=dataset_single, 
       alternative='two.sided',
       var.equal=T)
```

Important parameters: t and p-value (include in reports)

## For **Non Normal Distribution** but **Equal Variance**

### Multiple Comparisons

```{r kruskal.test}
library(dunn.test, quietly = T)
dunn.test(dataset_multiple$Value_normalised, 
          dataset_multiple$Condition, 
          altp=T,
          list=T)
```

Kruskal test finds there is any significant difference across the whole dataset. If the p-value is **ABOVE** 0.05 the analysis should be stopped here without comparing groups!

Important parameters: chi-squared and p-value (include in reports)

The dunn test displays both comparison matrix and comparison list of tested groups. P values are the individual p values between group combinations.

### Simple Comparison

```{r paired_t_test_2}
t.test(Value_normalised~Condition, 
       data= dataset_single, 
       alternative='two.sided',
       var.equal=T)
```

Important parameters: t and p-value (include in reports)

## For **Normal Distribution** but **Non Equal Variance**

### Multiple Comparisons

```{r one_way_test}
oneway.test(Value_normalised~Condition, 
            dataset_multiple, 
            var.equal = F)
```

One-way analysis of means finds there is any significant difference across the whole dataset. If the p-value is **ABOVE** 0.05 the analysis should be stopped here!

Important parameters: F value and Pr(>F) (include in reports)

```{r games_howell}
library(rstatix, quietly = T)
library(dplyr, quietly = T)
dataset_multiple %>% games_howell_test(Value_normalised~Condition)
```

P adj are the individual p values between group combinations.

### Single Comparison

```{r Welch_Two_Sample_t-test}
t.test(Value_normalised~Condition, 
       data=dataset_single, 
       alternative='two.sided',
       var.equal=F)
```

Important parameters: t and p-value (include in reports)